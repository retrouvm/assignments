{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 5 (100 + 20 points)\n",
    "* Download the file vvk160_hw5.ipynb\n",
    "* Open the file via Jupyter Notebook\n",
    "* Run the code by clicking the “Run” button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import sqrt\n",
    "from math import log2\n",
    "from csv import reader\n",
    "from random import seed\n",
    "from random import randrange\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.metrics import r2_score as r2\n",
    "import pdb #for debugging the code. Set a breakpoint by pbd.set_trace(). https://www.geeksforgeeks.org/python-debugger-python-pdb/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: Linear regression (20pts)\n",
    "* We do the linear regression on three points (0.5, 1), (2, 2.5), and (3, 3). \n",
    "* Please calculate the SSEs of the four following linear regression based on the definition SSE = ∑ ( y - ŷ )^2\n",
    "* Write the Python code to output the major steps of the calculation and results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = x + 0.5, SSE = 0.25\n",
      "y = x + 1, SSE = 1.5\n",
      "y = 0.8*x + 0.3 + 15, SSE = 0.54\n",
      "y = 0.8*x + 0.7, SSE = 0.06\n",
      "\n",
      "A least squares regression selects the line with the lowest total sum of squared prediction errors. Thus y = 0.8*x + 0.7 is the best linear regression\n"
     ]
    }
   ],
   "source": [
    "# Which is the best linear regression using SSE?\n",
    "\n",
    "#y = x + 0.5\n",
    "SSE1=((1 - (0.5+0.5))**2)+((2.5 - (2+0.5))**2) +((3- (3+0.5))**2)\n",
    "print(f\"y = x + 0.5, SSE = {SSE1}\")\n",
    "\n",
    "#y = x + 1\n",
    "SSE2=((1 - (0.5+1))**2)+((2.5 - (2+1))**2) +((3- (3+1))**2)\n",
    "print(f\"y = x + 1, SSE = {SSE2}\")\n",
    "\n",
    "#y = 0.8*x + 0.3\n",
    "SSE3=((1 - (0.8*0.5+0.3))**2)+((2.5 - (0.8*2+0.3))**2) +((3- (0.8*3+0.3))**2)\n",
    "print(f\"y = 0.8*x + 0.3 + 15, SSE = {round(SSE3,3)}\")\n",
    "\n",
    "#y = 0.8*x + 0.7\n",
    "SSE4=((1 - (0.8*0.5+0.7))**2)+((2.5 - (0.8*2+0.7))**2) +((3- (0.8*3+0.7))**2)\n",
    "print(f\"y = 0.8*x + 0.7, SSE = {round(SSE4,3)}\")\n",
    "\n",
    "print(\"\\nA least squares regression selects the line with the lowest total sum of squared prediction errors. Thus y = 0.8*x + 0.7 is the best linear regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: Lasso and Ridge regression (20pts)\n",
    "* What is the problem solved by Lasso and Ridge regression? What is the major\n",
    "difference between the two regression? Please discuss the advantages and disadvantages\n",
    "of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ridge and Lasso regression are some of the simple techniques to prevent overfitting in linear regression.\n",
      "\n",
      "Ridge regression shrinks the coefficients and it helps to reduce the model complexity and multi-collinearity (when independent variables are highly correlated)\n",
      "\n",
      "Lasso regression not only helps in reducing overfitting, but it can help us in feature selection.\n",
      "\n",
      "In ridge, the penalty is the sum of the squared coefficients\n",
      "In lasso, the penalty is the sum of the absolute values of coefficients\n",
      "\n",
      "Since Lasso Regression can exclude useless variables from equations, it is a little better than Ridge Regression at reducing the Variance in models that contain a lot of useless variables. In contrast, Ridge Regression tends to do a little better when most variables are useful.\n",
      "      \n"
     ]
    }
   ],
   "source": [
    "print('''\n",
    "Ridge and Lasso regression are some of the simple techniques to prevent overfitting in linear regression.\n",
    "\n",
    "Ridge regression shrinks the coefficients and it helps to reduce the model complexity and multi-collinearity (when independent variables are highly correlated)\n",
    "\n",
    "Lasso regression not only helps in reducing overfitting, but it can help us in feature selection.\n",
    "\n",
    "In ridge, the penalty is the sum of the squared coefficients\n",
    "In lasso, the penalty is the sum of the absolute values of coefficients\n",
    "\n",
    "Since Lasso Regression can exclude useless variables from equations, it is a little better than Ridge Regression at reducing the Variance in models that contain a lot of useless variables. In contrast, Ridge Regression tends to do a little better when most variables are useful.\n",
    "      ''')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: Decision Tree (30pts)\n",
    "* There are various ways to decide on the metric to choose the variable on which splitting for a node is done. Different algorithms deploy different metrices to decide which variable splits the dataset best.\n",
    "* Let’s say we have a sample of 30 records. There are two classes C1 and C2. We have three possible splits a, b, and c (see figure below). The number of records in each class is shown in every node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write a Python code to measure the node impurity using Gini Index\n",
    "\n",
    "#Gini of a_Left\n",
    "aLgC1 = 3/12\n",
    "aLgC2 = 9/12\n",
    "aLGini = 1 - aLgC1**2 - aLgC2**2\n",
    "#Gini of a_Right\n",
    "aRgC1 = 17/18\n",
    "aRgC2 = 1/18\n",
    "aRGini = 1 - aRgC1**2 - aRgC2**2\n",
    "\n",
    "#Gini of b_Left\n",
    "bLgC1 = 10/15\n",
    "bLgC2 = 5/15\n",
    "bLGini = 1 - bLgC1**2 - bLgC2**2\n",
    "#Gini of b_Right\n",
    "bRgC1 = 10/15\n",
    "bRgC2 = 5/15\n",
    "bRGini = 1 - bRgC1**2 - bRgC2**2\n",
    "\n",
    "#Gini of c_Left\n",
    "cLgC1 = 19/21\n",
    "cLgC2 = 2/21\n",
    "cLGini = 1 - cLgC1**2 - cLgC2**2\n",
    "#Gini of c_Right\n",
    "cRgC1 = 1/9\n",
    "cRgC2 = 8/9\n",
    "cRGini = 1 - cRgC1**2 - cRgC2**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write a Python code to measure the node impurity using Entropy Gain\n",
    "\n",
    "#Entropy Gain of a_Left\n",
    "aLeC1 = 3/12\n",
    "aLeC2 = 9/12\n",
    "aLEntropy = - aLeC1*log2(aLeC1) - aLeC2*log2(aLeC2)\n",
    "#Entropy Gain of a_Right\n",
    "aReC1 = 17/18\n",
    "aReC2 = 1/18\n",
    "aREntropy = - aReC1*log2(aReC1) - aReC2*log2(aReC2)\n",
    "\n",
    "#Entropy Gain of b_Left\n",
    "bLeC1 = 10/15\n",
    "bLeC2 = 5/15\n",
    "bLEntropy = - bLeC1*log2(bLeC1) - bLeC2*log2(bLeC2)\n",
    "#Entropy Gain of b_Right\n",
    "bReC1 = 10/15\n",
    "bReC2 = 5/15\n",
    "bREntropy = - bReC1*log2(bReC1) - bReC2*log2(bReC2)\n",
    "\n",
    "#Entropy Gain of c_Left\n",
    "cLeC1 = 19/21\n",
    "cLeC2 = 2/21\n",
    "cLEntropy = - cLeC1*log2(cLeC1) - cLeC2*log2(cLeC2)\n",
    "#Entropy Gain of c_Right\n",
    "cReC1 = 1/9\n",
    "cReC2 = 8/9\n",
    "cREntropy = - cReC1*log2(cReC1) - cReC2*log2(cReC2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write a Python code to measure the node impurity using Misclassification Error\n",
    "\n",
    "#Error of a_Left\n",
    "aLmC1 = 3/12\n",
    "aLmC2 = 9/12\n",
    "aLMisclass = 1 - max(aLgC1,aLgC2)\n",
    "#Error of a_Right\n",
    "aRmC1 = 17/18\n",
    "aRmC2 = 1/18\n",
    "aRMisclass = 1 - max(aRmC1,aRmC2)\n",
    "\n",
    "#Error of b_Left\n",
    "bLmC1 = 10/15\n",
    "bLmC2 = 5/15\n",
    "bLMisclass = 1 - max(bLmC1,bLmC2)\n",
    "#Error of b_Right\n",
    "bRmC1 = 10/15\n",
    "bRmC2 = 5/15\n",
    "bRMisclass = 1 - max(bRmC1,bRmC2)\n",
    "\n",
    "#Error of c_Left\n",
    "cLmC1 = 19/21\n",
    "cLmC2 = 2/21\n",
    "cLMisclass = 1 - max(cLmC1,cLmC2)\n",
    "#Error of c_Right\n",
    "cRmC1 = 1/9\n",
    "cRmC2 = 8/9\n",
    "cRMisclass = 1 - max(cRmC1,cRmC2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node impurities:\n",
      "Gini index: a-left: 0.375; right: 0.105; b-left: 0.444; right: 0.444; c-left: 0.172; right: 0.198;\n",
      "Entropy Gain: a-left: 0.811; right: 0.31; b-left: 0.918; right: 0.918; c-left: 0.454; right: 0.503;\n",
      "Misclassification Error: a-left: 0.25; right: 0.13043; b-left: 0.4; right: 0.4; c-left: 0.2; right: 0.33333;\n"
     ]
    }
   ],
   "source": [
    "#Measure the node impurity using Gini Index, Entropy Gain, and Misclassification Error, respectively\n",
    "print(f'''Node impurities:\n",
    "Gini index: a-left: {round(aLGini,3)}; right: {round(aRGini,3)}; b-left: {round(bLGini,3)}; right: {round(bRGini,3)}; c-left: {round(cLGini,3)}; right: {round(cRGini,3)};\n",
    "Entropy Gain: a-left: {round(aLEntropy,3)}; right: {round(aREntropy,3)}; b-left: {round(bLEntropy,3)}; right: {round(bREntropy,3)}; c-left: {round(cLEntropy,3)}; right: {round(cREntropy,3)};\n",
    "Misclassification Error: a-left: {round(aLMisclass,5)}; right: {round(aRMisclass,5)}; b-left: {round(bLMisclass,5)}; right: {round(bRMisclass,5)}; c-left: {round(cLMisclass,5)}; right: {round(cRMisclass,5)};''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassification Error: a-left: 0.25; right: 0.25; b-left: 0.25\n"
     ]
    }
   ],
   "source": [
    "#Error of a parent\n",
    "aC1 = 30/40\n",
    "aC2 = 10/40\n",
    "aMisclass = 1 - max(aC1,aC2)\n",
    "#Error of b parent\n",
    "bC1 = 30/40\n",
    "bC2 = 10/40\n",
    "bMisclass = 1 - max(bC1,bC2)\n",
    "#Error of c parent\n",
    "cC1 = 30/40\n",
    "cC2 = 10/40\n",
    "cMisclass = 1 - max(cC1,cC2)\n",
    "print(f\"Misclassification Error: a-left: {round(aMisclass,3)}; right: {round(bMisclass,3)}; b-left: {round(cMisclass,3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gini\n",
    "Gini_Childrena = 12/30 * aLGini + 18/30 * aRGini\n",
    "Gini_Childrenb = 15/30 * bLGini + 15/30 * bRGini\n",
    "Gini_Childrenc = 21/30 * cLGini + 9/30 * cRGini\n",
    "\n",
    "#entropy\n",
    "Entropy_P = - (20/30)*log2(20/30) - (10/30)*log2(10/30)\n",
    "E_remainder1=(18/30 * aREntropy)+(12/30 * aLEntropy)\n",
    "Entropy_Childrena =Entropy_P -E_remainder1\n",
    "\n",
    "E_remainder2=(15/30 * bLEntropy)+(15/30 * bREntropy)\n",
    "Entropy_Childrenb =Entropy_P -E_remainder2\n",
    "\n",
    "E_remainder3=(21/30 * cLEntropy)+(9/30 * cREntropy)\n",
    "Entropy_Childrenc =Entropy_P -E_remainder3\n",
    "\n",
    "#error\n",
    "Error_P=1 - max(20/30,10/30)\n",
    "M_remainder1=(12/30 * aLMisclass)+(18/30 * aRMisclass)\n",
    "Error_Childrena=Error_P-M_remainder1\n",
    "\n",
    "M_remainder2=(15/30 * bLMisclass)+(15/30 * bRMisclass)\n",
    "Error_Childrenb=Error_P-M_remainder2\n",
    "\n",
    "M_remainder3=(21/30 * cLMisclass)+(9/30 * cRMisclass)\n",
    "Error_Childrenc=Error_P-M_remainder3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting quality:\n",
      "Gini index: a: 0.213; b: 0.444; c: 0.18;\n",
      "Entropy Gain: a: 0.408; b: 0.0; c: 0.45;\n",
      "Misclassification Error: a: 0.06875; b: -0.15; c: 0.0;\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the quality of the three splitting and report the best one (a, b, or c)\n",
    "print(f'''Splitting quality:\n",
    "Gini index: a: {round(Gini_Childrena,3)}; b: {round(Gini_Childrenb,3)}; c: {round(Gini_Childrenc,3)};\n",
    "Entropy Gain: a: {round(Entropy_Childrena,3)}; b: {Entropy_Childrenb}; c: {round(Entropy_Childrenc,3)};\n",
    "Misclassification Error: a: {round(Error_Childrena,5)}; b: {round(Error_Childrenb,5)}; c: {round(Error_Childrenc,5)};''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All three methods have the same best splitting: c\n"
     ]
    }
   ],
   "source": [
    "#print out your conclusion about whether all three methods have the same best splitting or not\n",
    "print('All three methods have the same best splitting: c')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4: KNN (30pts)\n",
    "*  KNN: this section applies the KNN algorithm to the Iris flowers dataset.\n",
    "* The first step is to load the dataset in “iris.csv” and convert the loaded data to numbers that we can use with the mean and standard deviation calculations.\n",
    "* For this we will use the helper function load_csv() to load the file, str_column_to_float() to convert string numbers to floats and str_column_to_int() to convert the class column to integer values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5: Pruning of decision tree (extra credit: 20pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Error (Before splitting) = 10/30 => 0.34\n",
      "Pessimistic error = (10 + 0.5)/30 = 10.5/30 => 0.35\n",
      "Training Error (After splitting) = 9/30 => 0.30\n",
      "Pessimistic error (After splitting) = (9 + 4 x 0.5)/30 = 0.37\n",
      "This tree should be pruned since 0.37>0.35\n",
      "      \n"
     ]
    }
   ],
   "source": [
    "print('''\n",
    "Training Error (Before splitting) = 10/30 => 0.34\n",
    "Pessimistic error = (10 + 0.5)/30 = 10.5/30 => 0.35\n",
    "Training Error (After splitting) = 9/30 => 0.30\n",
    "Pessimistic error (After splitting) = (9 + 4 x 0.5)/30 = 0.37\n",
    "This tree should be pruned since 0.37>0.35\n",
    "      ''')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "51e4f816afd506db7bb7650607a3e2e026e40398beddcc9e11824953f6978b8d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
